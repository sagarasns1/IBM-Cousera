{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Assignment 4\n## Understaning scaling of linear algebra operations on Apache Spark using Apache SystemML\n\nIn this assignment we want you to understand how to scale linear algebra operations from a single machine to multiple machines, memory and CPU cores using Apache SystemML. Therefore we want you to understand how to migrate from a numpy program to a SystemML DML program. Don't worry. We will give you a lot of hints. Finally, you won't need this knowledge anyways if you are sticking to Keras only, but once you go beyond that point you'll be happy to see what's going on behind the scenes. As usual, we run some import statements:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 2, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'2.1.3'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "sc.version"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already up-to-date: systemml in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s162-93eff8e5c01afa-abeaff432b02/.local/lib/python3.5/site-packages (1.2.0)\nRequirement not upgraded as not directly required: numpy>=1.8.2 in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (1.13.3)\nRequirement not upgraded as not directly required: Pillow>=2.0.0 in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (4.2.1)\nRequirement not upgraded as not directly required: scipy>=0.15.1 in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (1.0.0)\nRequirement not upgraded as not directly required: scikit-learn in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (0.19.1)\nRequirement not upgraded as not directly required: pandas in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from systemml) (0.21.0)\nRequirement not upgraded as not directly required: olefile in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from Pillow>=2.0.0->systemml) (0.44)\nRequirement not upgraded as not directly required: python-dateutil>=2 in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->systemml) (2.6.1)\nRequirement not upgraded as not directly required: pytz>=2011k in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->systemml) (2018.4)\nRequirement not upgraded as not directly required: six>=1.5 in /usr/local/src/conda3_runtime.v47/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from python-dateutil>=2->pandas->systemml) (1.11.0)\n\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n"
                }
            ], 
            "source": "!pip install --upgrade systemml"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from systemml import MLContext, dml\nimport numpy as np\nimport time"
        }, 
        {
            "source": "Then we create an MLContext to interface with Apache SystemML. Note that we pass a SparkSession object as parameter so SystemML now knows how to talk to the Apache Spark cluster", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ml = MLContext(spark)"
        }, 
        {
            "source": "Now we create some large random matrices to have numpy and SystemML crunch on it", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "u = np.random.rand(1000,10000)\ns = np.random.rand(10000,1000)\nw = np.random.rand(1000,1000)"
        }, 
        {
            "source": "Now we implement a short one-liner to define a very simple linear algebra operation\n\nIn case you are not familiar with matrix-matrix multiplication: https://en.wikipedia.org/wiki/Matrix_multiplication\n\nsum(U' * (W . (U * S)))\n\n\n| Legend        |            |   \n| ------------- |-------------| \n| '      | transpose of a matrix | \n| * | matrix-matrix multiplication      |  \n| . | scalar multiplication      |   \n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.45916199684143066\n"
                }
            ], 
            "source": "start = time.time()\nres = np.sum(u.T.dot(w * u.dot(s)))\nprint (time.time()-start)"
        }, 
        {
            "source": "As you can see this executes perfectly fine. Note that this is even a very efficient execution because numpy uses a C/C++ backend which is known for it's performance. But what happens if U, S or W get such big that the available main memory cannot cope with it? Let's give it a try:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "u = np.random.rand(10000,100000)\ns = np.random.rand(100000,10000)\nw = np.random.rand(10000,10000)"
        }, 
        {
            "source": "After a short while you should see a memory error. This is because the operating system process was not able to allocate enough memory for storing the numpy array on the heap. Now it's time to re-implement the very same operations as DML in SystemML, and this is your task. Just replace all ###your_code_goes_here sections with proper code, please consider the following table which contains all DML syntax you need:\n\n| Syntax        |            |   \n| ------------- |-------------| \n| t(M)      | transpose of a matrix, where M is the matrix | \n| %*% | matrix-matrix multiplication      |  \n| * | scalar multiplication      |   \n\n## Task", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 41, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "script = \"\"\"\nres = sum(t(U))\n\"\"\""
        }, 
        {
            "execution_count": 50, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "script = \"\"\"\nres = sum(t(U) %*% (W * (U %*% S))\n\"\"\""
        }, 
        {
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "script = \"\"\"\nres = sum((U %*% S))\n\"\"\""
        }, 
        {
            "source": "script = \"\"\"\nres = sum(###your_code_goes_here(U) %*% (W * (U ###your_code_goes_here S)))\n\"\"\"", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To get consistent results we switch from a random matrix initialization to something deterministic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "u = np.arange(100000).reshape((100, 1000))\ns = np.arange(100000).reshape((1000, 100))\nw = np.arange(10000).reshape((100, 100))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prog = dml(script).input('U', u).input('S', s).output('res')\nres = ml.execute(prog).get('res')\nprint (res)"
        }, 
        {
            "source": "prog = dml(script).input('U', u).input('S', s).input('W', w).output('res')\nres = ml.execute(prog).get('res')\nprint (res)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "If everything runs fine you should get *1.25260525922e+28* as result. Feel free to submit your DML script to the grader now!\n\n### Submission", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from rklib import submit\nkey = \"esRk7vn-Eeej-BLTuYzd0g\"\npart = \"fUxc8\"\n\nemail = \"###_YOUR_CODE_GOES_HERE_###\"\nsecret = \"###_YOUR_CODE_GOES_HERE_###\"\nsubmit(email, secret, key, part, [part], script)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}